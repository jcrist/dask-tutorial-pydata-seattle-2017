{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Schedulers\n",
    "\n",
    "In the previous two notebooks we used Dask.delayed and Dask.dataframe to create computations.  By default, these ran in a local thread pool on our personal machines.  Often, this is sufficient, especially when you are bound by NumPy and Pandas routines which release the GIL and when you are using powerful workstation computers with many cores.\n",
    "\n",
    "However sometimes you may want to execute your code in processes (for Pure Python code that holds onto the GIL), in a single thread (for profiling and debugging) or across a cluster (for larger computations).\n",
    "\n",
    "In this section we first talk about changing schedulers.  Then we use the `dask.distributed` scheduler in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Schedulers\n",
    "\n",
    "Dask separates computation description (task graphs) from execution (schedulers). This allows you to write code once, and run it locally or scale it out across a cluster.\n",
    "\n",
    "Here we discuss the *local* schedulers - schedulers that run only on a single machine. The three options here are:\n",
    "\n",
    "- `dask.threaded.get         # uses a local thread pool`\n",
    "- `dask.multiprocessing.get  # uses a local process pool`\n",
    "- `dask.get                  # uses only the main thread (useful for debugging)`\n",
    "\n",
    "In each case we change the scheduler used in a few different ways:\n",
    "\n",
    "- By providing a `get=` keyword argument to `compute`:\n",
    "\n",
    "```python\n",
    "total.compute(get=dask.multiprocessing.get)\n",
    "# or \n",
    "dask.compute(a, b, get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "- Using `dask.set_options`:\n",
    "\n",
    "```python\n",
    "# Use multiprocessing in this block\n",
    "with dask.set_options(get=dask.multiprocessing.get):\n",
    "    total.compute()\n",
    "# Use multiprocessing globally\n",
    "dask.set_options(get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "Here we repeat a simple dataframe computation from the previous section using the different schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "import dask.multiprocessing\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': object,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "# Maximum non-cancelled delay\n",
    "largest_delay = df[~df.Cancelled].DepDelay.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # this uses threads by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.multiprocessing.get)  # this uses processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.get)  # This uses a single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the threaded and multiprocessing schedulers use the same number of workers as cores. You can change this using the `num_workers` keyword in the same way that you specified `get` above:\n",
    "\n",
    "```\n",
    "largest_delay.compute(get=dask.multiprocessing.get, num_workers=2)\n",
    "```\n",
    "\n",
    "To see how many cores you have on your computer, you can use `multiprocessing.cpu_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Questions to Consider:\n",
    "\n",
    "- How much speedup is possible for this task (hint, look at the graph).\n",
    "- Given how many cores are on this machine, how much faster could the parallel schedulers be than the single-threaded scheduler.\n",
    "- How much faster was using threads over a single thread? Why does this differ from the optimal speedup?\n",
    "- Why is the multiprocessing scheduler so much slower here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what cases would you want to use one scheduler over another?\n",
    "\n",
    "http://dask.pydata.org/en/latest/scheduler-choice.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "*You should skip this section if you are running low on time*.\n",
    "\n",
    "The synchronous scheduler is particularly valuable for debugging and profiling.  \n",
    "\n",
    "For example, the IPython `%%prun` magic gives us profiling information about which functions take up the most time in our computation.  Try this magic on the computation above with different schedulers.  How informative is this magic when running parallel code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun _ = largest_delay.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun _ = largest_delay.compute(get=dask.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in profiling parallel execution, dask provides several [`diagnostics`](http://dask.pydata.org/en/latest/diagnostics.html) for measuring and visualizing performance. These are useful for seeing bottlenecks in the *parallel* computation, whereas the above `prun` is useful for seeing bottlenecks in individual *tasks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler, visualize\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "with Profiler() as p, ResourceProfiler(0.25) as r:\n",
    "    largest_delay.compute()\n",
    "    \n",
    "visualize([r, p]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that while tasks are running concurrently, due to GIL effects we're only achieving parallelism during early parts of `pd.read_csv` (mostly the byte operations).\n",
    "\n",
    "\n",
    "*It should be noted that the `dask.diagnostics` module is only useful when profiling on a single machine. The `dask.distributed` scheduler has its own set of diagnostics..*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and several worker processes.  We can either set these up manually as command line processes or have Dask set them up for us from the notebook.  \n",
    "\n",
    "\n",
    "#### Automatically setup a local cluster\n",
    "\n",
    "Starting a single scheduler and worker on the local machine is a common case. Dask will set up a local cluster for you if you provide no scheduler address to `Client`:\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "If you choose this approach then there is no need to set up a `dask-scheduler` or `dask-worker` process as described below.\n",
    "\n",
    "\n",
    "#### Manually setup a distributed cluster\n",
    "\n",
    "It is good to know how to set things up manually in case you want to try out Dask on a small cluster of your own.  However, if you are unfamiliar with the command line you can safely skip this section and go down to the Automatic section below. We run the `dask-scheduler` process on one machine.\n",
    "\n",
    "```\n",
    "$ dask-scheduler\n",
    "distributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n",
    "distributed.bokeh.application - INFO - Web UI: http://127.0.0.1:8787/status/\n",
    "```\n",
    "\n",
    "The scheduler reports that it is running at 127.0.0.1 on port 8786.  The address 127.0.0.1 is another name for \"localhost\" or \"this machine\".  We will need to give this address to the workers and client so you might want to copy it now.\n",
    "\n",
    "We now run the `dask-worker` process on every machine that we want to use for computation.  For right now this is probably just once on our laptop, but in production this may be on many different machines:\n",
    "\n",
    "```\n",
    "$ dask-worker 127.0.0.1:8786\n",
    "distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45011\n",
    "distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:8786\n",
    "```\n",
    "\n",
    "*Note*: for this tutorial we want to start the dask-worker process in the `dask-tutorial-pydata-seattle/` directory.  This will ensure that the workers Python processes have access to the same data that our notebook process does.\n",
    "\n",
    "```\n",
    "$ cd dask-tutorial-pydata-seattle/  # navigate to whereever you have started your notebooks\n",
    "~/dask-tutorial-pydata-seattle/ $ dask-worker 127.0.0.1:8786\n",
    "```\n",
    "\n",
    "*Note*: By default the dask-worker command line tool starts a single process with a thread pool with as many threads as you have cores on your computer.  If you are doing mostly GIL-released computations (numpy, pandas, scikit-learn) then this is the right choice.  However if you are doing mostly GIL-bound comptutations (Python code, pandas with text, parsing) then you will want to start the workers with multiple processes and one thread per process\n",
    "\n",
    "```\n",
    "$ dask-worker 127.0.0.1:8786 --nprocs 8 --nthreads 4\n",
    "```\n",
    "    \n",
    "You can see more options by asking for help\n",
    "\n",
    "```\n",
    "$ dask-worker --help\n",
    "```\n",
    "    \n",
    "When the scheduler and workers are running you can connect to them using a Dask `Client`, giving it the same address of the scheduler that you gave to the worker.\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "```\n",
    "\n",
    "### More information\n",
    "\n",
    "You can find more information at the following documentation pages:\n",
    "\n",
    "- [Quickstart](http://distributed.readthedocs.io/en/latest/quickstart.html)\n",
    "- [Setup Network](http://distributed.readthedocs.io/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a local cluster\n",
    "\n",
    "As mentioned above, the multiprocessing scheduler can be inefficient for complicated workflows. The distributed scheduler doesn't have this downside, and works fine locally. This makes it often a good replacement for the multiprocessing scheduler, even when working on a single machine.\n",
    "\n",
    "Here we startup a local cluster, and use it to repeat the same dataframe computation as done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per core\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=client.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Questions to Consider\n",
    "\n",
    "- How does this compare to the optimal parallel speedup?\n",
    "- Why is this faster than the threaded scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client takes over by default\n",
    "\n",
    "Actually, we didn't need to add `get=client.get`.  The distributed scheduler takes over as the default scheduler for all collections when the Client is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # This used to use threads by default, now it uses dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "One of the main advantages of using the distributed scheduler is the diagnostics dashboards that should be hosted live at http://localhost:8787/status .  Visit that link and then run the computation again.\n",
    "\n",
    "You may want to arrange your notebook and this webpage side-by-side on your screen so that you can see both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the groupby computation from the previous notebook, and watch the diagnostics page. What is taking all of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the average departure delay from each airport?\n",
    "df[~df.Cancelled].groupby('Origin').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New API\n",
    "\n",
    "The distributed scheduler is more sophisticated than the single machine schedulers.  It comes with more functions to manage data, computing in the background, and more.  The distributed scheduler also has entirely separate documentation\n",
    "\n",
    "-  http://distributed.readthedocs.io/en/latest/\n",
    "-  http://distributed.readthedocs.io/en/latest/api.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
